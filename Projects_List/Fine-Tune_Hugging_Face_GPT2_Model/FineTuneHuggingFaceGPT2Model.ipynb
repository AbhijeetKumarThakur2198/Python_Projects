{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM2xZXUYihJluHxBLYU1umO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhijeetKumarThakur2198/Python_Projects/blob/main/Projects_List/Fine-Tune_Hugging_Face_GPT2_Model/FineTuneHuggingFaceGPT2Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we will fine-tune the Hugging Face GPT2 model using the transformers trainer function."
      ],
      "metadata": {
        "id": "vfLJ_KJz8Evo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies\n",
        "!pip install transformers torch accelerate"
      ],
      "metadata": {
        "id": "8ACJaDMBfT0L",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Fine-Tune GPT2 Model\n",
        "#@markdown Fill Hyperparameters\n",
        "model_name = \"gpt2\"  #@param [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"]\n",
        "file_path = \"data.txt\"  #@param  {type:\"string\"}\n",
        "output_path = \"Fine_Tuned_Model\"  #@param  {type:\"string\"}\n",
        "batch_size = 16  #@param  {type:\"integer\"}\n",
        "num_epochs = 50  #@param  {type:\"integer\"}\n",
        "block_size = 124  #@ param {type:\"integer\"}\n",
        "learning_rate = 5e-5  #@param  {type:\"number\"}\n",
        "save_steps = 10000  #@param  {type:\"integer\"}\n",
        "overwrite_output_path = True  #@param  {type:\"boolean\"}\n",
        "\n",
        "# IMPORT NECESSARY MODULES\n",
        "try:\n",
        "    import re\n",
        "    import os\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Pytorch module not found in your environment please download it!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "try:\n",
        "    from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Transformers module not found in your environment please download it!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# DEFINE FUNCTIONS\n",
        "def read_txt(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def load_dataset(file_path, tokenizer, block_size):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=file_path,\n",
        "        block_size=block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "def fine_tune(train_file_path, model_name, output_path, overwrite_output_path, per_device_train_batch_size, num_train_epochs, save_steps, learning_rate, block_size):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    text_data = read_txt(train_file_path)\n",
        "    text_data = re.sub(r'\\n+', '\\n', text_data).strip()\n",
        "    train_dataset = load_dataset(train_file_path, tokenizer, block_size)\n",
        "\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "\n",
        "    model.save_pretrained(output_path)\n",
        "    tokenizer.save_pretrained(output_path)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_path,\n",
        "        overwrite_output_dir=overwrite_output_path,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        save_steps=save_steps,\n",
        "        learning_rate=learning_rate,\n",
        "        do_train=True\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        "            train_dataset=train_dataset\n",
        "        )\n",
        "        trainer.train()\n",
        "        trainer.save_model()\n",
        "\n",
        "        print(\"Your fine-tuning process is completed. Now you can use your fine-tuned model.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Oh! It seems like something went wrong: {str(e)}. Please check all information again or open GitHub issue!\")\n",
        "\n",
        "# FINE TUNE MODEL\n",
        "fine_tune(\n",
        "    train_file_path=file_path,\n",
        "    model_name=model_name,\n",
        "    output_path=output_path,\n",
        "    overwrite_output_path=overwrite_output_path,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    save_steps=save_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    block_size=block_size\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HrZOKdFa1M_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Inference Fine-Tuned Model\n",
        "#@markdown Fill Configuration\n",
        "import torch\n",
        "max_length = 50  #@param {type:\"integer\"}\n",
        "temperature = 0.7  #@param {type:\"number\"}\n",
        "return_tensors = \"pt\"  #@param {type:\"string\"}\n",
        "num_return_sequences = 1  #@param {type:\"integer\"}\n",
        "skip_special_tokens = True  #@param {type:\"boolean\"}\n",
        "seed_text = \"Once upon a time\"  #@param {type:\"string\"}\n",
        "Fine_Tuned_Model_Path = \"Fine_Tuned_Model\"  #@param {type:\"string\"}\n",
        "\n",
        "# IMPORT NECCESARY MODULES\n",
        "try:\n",
        "    import torch\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Pytorch module not found in your environment please download it!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "try:\n",
        "    from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "except ModuleNotFoundError:\n",
        "    print(\"transformers module not found in your environment please download it!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# LOAD MODEL\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(Fine_Tuned_Model_Path)\n",
        "model = GPT2LMHeadModel.from_pretrained(Fine_Tuned_Model_Path)\n",
        "\n",
        "# ENCODE SEED TEXT INTO INTEGERS\n",
        "encoded_text = tokenizer.encode(seed_text, return_tensors=return_tensors)\n",
        "\n",
        "# Create attention mask\n",
        "attention_mask = torch.ones(encoded_text.shape, dtype=torch.long)\n",
        "\n",
        "# GENERATE OUTPUT\n",
        "output = model.generate(\n",
        "    encoded_text,\n",
        "    max_length=max_length,\n",
        "    num_return_sequences=num_return_sequences,\n",
        "    temperature=temperature,\n",
        "    do_sample=True,\n",
        "    attention_mask=attention_mask,  # Pass attention mask\n",
        "    pad_token_id=tokenizer.eos_token_id  # Set pad token ID\n",
        ")\n",
        "\n",
        "# DECODE INTEGERS TO STRINGS AND PRINT\n",
        "decoded_text = tokenizer.decode(output[0], skip_special_tokens=skip_special_tokens)\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "id": "o0SSAJutJRm3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}