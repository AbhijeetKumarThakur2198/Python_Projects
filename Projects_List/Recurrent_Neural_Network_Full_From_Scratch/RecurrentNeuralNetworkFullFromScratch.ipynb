{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9a9QPSeDggPpcVt2j7ngn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhijeetKumarThakur2198/Python_Projects/blob/main/Projects_List/Recurrent_Neural_Network_Full_From_Scratch/RecurrentNeuralNetworkFullFromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Recurrent Neural Network (RNN) project has been constructed completely from scratch, without relying on any third-party modules. It solely utilizes pre-built modules!"
      ],
      "metadata": {
        "id": "8iGOmm88Rrv_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fQj0p8Q-Px0a"
      },
      "outputs": [],
      "source": [
        "#@title RNN Architecture\n",
        "# IMPORT PRE-BUILT MODULES\n",
        "import random\n",
        "import math\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "# DEFINE FUNCTIONS FOR INITIALIZING WEIGHT MATRICES AND VECTORS\n",
        "def initialize_weight_matrix(rows, cols):\n",
        "    return [[random.uniform(-math.sqrt(1. / rows), math.sqrt(1. / rows)) for _ in range(rows)] for _ in range(cols)]\n",
        "\n",
        "def initialize_column_vector(a):\n",
        "    return [[0] for _ in range(a)]\n",
        "\n",
        "def initialize_zero_matrix(rows, cols):\n",
        "    return [[0] * rows for _ in range(cols)]\n",
        "\n",
        "# RECURRENT NEURAL NETWORK ARCHITECTURE\n",
        "class RecurrentNeuralNetwork:\n",
        "    def __init__(self, config_path):\n",
        "        with open(config_path, \"r\") as f:\n",
        "            config_data = json.load(f)\n",
        "\n",
        "        # INITIALIZE NETWORK PARAMETERS BASED ON CONFIGURATION\n",
        "        self.hidden_size = config_data[\"hidden_size\"]\n",
        "        self.vocab_size = config_data[\"vocab_size\"]\n",
        "        self.sequence_length = config_data[\"sequence_length\"]\n",
        "        self.learning_rate = config_data[\"learning_rate\"]\n",
        "        self.convergence_threshold = config_data[\"convergence_threshold\"]\n",
        "        self.U = initialize_weight_matrix(self.vocab_size, self.hidden_size)\n",
        "        self.V = initialize_weight_matrix(self.hidden_size, self.vocab_size)\n",
        "        self.W = initialize_weight_matrix(self.hidden_size, self.hidden_size)\n",
        "        self.bias = initialize_column_vector(self.hidden_size)\n",
        "        self.output_bias = initialize_column_vector(self.vocab_size)\n",
        "        self.memory_U = initialize_zero_matrix(self.vocab_size, self.hidden_size)\n",
        "        self.memory_W = initialize_zero_matrix(self.hidden_size, self.hidden_size)\n",
        "        self.memory_V = initialize_zero_matrix(self.hidden_size, self.vocab_size)\n",
        "        self.memory_bias = initialize_column_vector(self.hidden_size)\n",
        "        self.memory_output_bias = initialize_column_vector(self.vocab_size)\n",
        "\n",
        "    # METHOD TO COMPUTE SOFTMAX ACTIVATION\n",
        "    def softmax(self, x):\n",
        "        probabilities = [math.exp(xi - max(x)) for xi in x]\n",
        "        return [pi / sum(probabilities) for pi in probabilities]\n",
        "\n",
        "    # FORWARD PASS THROUGH THE NETWORK\n",
        "    def forward_pass(self, inputs, previous_hidden_state):\n",
        "        # INITIALIZE DICTIONARIES TO STORE INTERMEDIATE STATES AND OUTPUTS\n",
        "        input_states, hidden_states, output_states, predicted_output = {}, {}, {}, {}\n",
        "        hidden_states[-1] = previous_hidden_state[:] # INITIALIZE PREVIOUS HIDDEN STATE\n",
        "\n",
        "        # ITERATE OVER EACH TIME STEP\n",
        "        for time_step in range(len(inputs)):\n",
        "            # COMPUTE INPUT, HIDDEN, AND OUTPUT STATES\n",
        "            input_states[time_step] = [0] * self.vocab_size\n",
        "            input_states[time_step][inputs[time_step]] = 1\n",
        "            hidden_states[time_step] = [math.tanh(\n",
        "                sum(self.U[i][j] * input_states[time_step][j] for j in range(self.vocab_size)) +\n",
        "                sum(self.W[i][j] * hidden_states[time_step - 1][j] for j in range(self.hidden_size)) +\n",
        "                self.bias[i][0]) for i in range(self.hidden_size)]\n",
        "            output_states[time_step] = [sum(self.V[i][j] * hidden_states[time_step][j] for j in range(self.hidden_size)) +\n",
        "                                        self.output_bias[i][0] for i in range(self.vocab_size)]\n",
        "            predicted_output[time_step] = self.softmax(output_states[time_step])\n",
        "        return input_states, hidden_states, predicted_output\n",
        "\n",
        "    # BACKWARD PASS THROUGH THE NETWORK\n",
        "    def backward_pass(self, input_states, hidden_states, predicted_output, targets):\n",
        "        # INITIALIZE GRADIENT MATRICES AND VECTORS\n",
        "        dU, dW, dV = [[0] * self.vocab_size for _ in range(self.hidden_size)], [[0] * self.hidden_size for _ in\n",
        "                                                                              range(self.hidden_size)], [\n",
        "                         [0] * self.hidden_size for _ in range(self.vocab_size)]\n",
        "        dbias, d_output_bias = [[0] for _ in range(self.hidden_size)], [[0] for _ in range(self.vocab_size)]\n",
        "        hidden_state_update = [0] * self.hidden_size\n",
        "\n",
        "        # ITERATE OVER EACH TIME STEP IN REVERSE ORDER\n",
        "        for time_step in reversed(range(self.sequence_length)):\n",
        "            output_error = predicted_output[time_step][:]\n",
        "            output_error[targets[time_step]] -= 1\n",
        "\n",
        "            for i in range(self.vocab_size):\n",
        "                for j in range(self.hidden_size):\n",
        "                    dV[i][j] += output_error[i] * hidden_states[time_step][j]\n",
        "                d_output_bias[i][0] += output_error[i]\n",
        "            hidden_error = [sum(self.V[i][j] * output_error[i] for i in range(self.vocab_size)) + hidden_state_update[j]\n",
        "                            for j in range(self.hidden_size)]\n",
        "            hidden_state_recurrent_error = [(1 - hidden_states[time_step][j] * hidden_states[time_step][j]) *\n",
        "                                            hidden_error[j] for j in range(self.hidden_size)]\n",
        "\n",
        "            for i in range(self.hidden_size):\n",
        "                dbias[i][0] += hidden_state_recurrent_error[i]\n",
        "                for j in range(self.vocab_size):\n",
        "                    dU[i][j] += hidden_state_recurrent_error[i] * input_states[time_step][j]\n",
        "                for j in range(self.hidden_size):\n",
        "                    dW[i][j] += hidden_state_recurrent_error[i] * hidden_states[time_step - 1][j]\n",
        "            hidden_state_update = [sum(self.W[i][j] * hidden_state_recurrent_error[j] for j in range(self.hidden_size))\n",
        "                                   for i in range(self.hidden_size)]\n",
        "\n",
        "        # UPDATE PARAMETERS USING RMSPROP OPTIMIZATION\n",
        "        for parameter_updates, parameters, memory in zip([dU, dW, dV, dbias, d_output_bias],\n",
        "                                                        [self.U, self.W, self.V, self.bias, self.output_bias],\n",
        "                                                        [self.memory_U, self.memory_W, self.memory_V, self.memory_bias,\n",
        "                                                         self.memory_output_bias]):\n",
        "            for i in range(len(parameters)):\n",
        "                for j in range(len(parameters[0])):\n",
        "                    parameter_updates[i][j] = max(-5, min(5, parameter_updates[i][j]))\n",
        "                    memory[i][j] += parameter_updates[i][j] * parameter_updates[i][j]\n",
        "                    parameters[i][j] += -self.learning_rate * parameter_updates[i][j] / math.sqrt(\n",
        "                        memory[i][j] + 1e-8)\n",
        "        return dU, dW, dV, dbias, d_output_bias\n",
        "\n",
        "    # METHOD TO CALCULATE LOSS\n",
        "    def calculate_loss(self, predicted_output, targets):\n",
        "        return sum(-math.log(predicted_output[time_step][targets[time_step]]) for time_step in\n",
        "                   range(self.sequence_length))\n",
        "\n",
        "    # METHOD TO SAVE MODEL PARAMETERS\n",
        "    def save_model(self, save_path):\n",
        "        model_data = {\n",
        "            \"U\": self.U,\n",
        "            \"V\": self.V,\n",
        "            \"W\": self.W,\n",
        "            \"bias\": self.bias,\n",
        "            \"output_bias\": self.output_bias\n",
        "        }\n",
        "        with open(save_path, \"wb\") as file:\n",
        "            pickle.dump(model_data, file)\n",
        "\n",
        "    # METHOD TO LOAD MODEL PARAMETERS\n",
        "    def load_model(self, file_path):\n",
        "        with open(file_path, \"rb\") as file:\n",
        "            model_data = pickle.load(file)\n",
        "\n",
        "        self.U = model_data[\"U\"]\n",
        "        self.V = model_data[\"V\"]\n",
        "        self.W = model_data[\"W\"]\n",
        "        self.bias = model_data[\"bias\"]\n",
        "        self.output_bias = model_data[\"output_bias\"]\n",
        "\n",
        "    # METHOD FOR GENERATING TEXT BASED ON SEED TEXT(INFERENCE FUNCTION)\n",
        "    def generate(self, seed_text, generated_length):\n",
        "        input_state = [0] * self.vocab_size\n",
        "        characters = [a for a in seed_text]\n",
        "        generated_indices = []\n",
        "\n",
        "        for i in range(len(characters)):\n",
        "            index = seed_text[i]\n",
        "            input_state[index] = 1\n",
        "            generated_indices.append(index)\n",
        "        hidden_state = [0] * self.hidden_size\n",
        "\n",
        "        for time_step in range(generated_length):\n",
        "            hidden_state = [math.tanh(\n",
        "                sum(self.U[i][j] * input_state[j] for j in range(self.vocab_size)) +\n",
        "                sum(self.W[i][j] * hidden_state[j] for j in range(self.hidden_size)) +\n",
        "                self.bias[i][0]) for i in range(self.hidden_size)]\n",
        "            output = [sum(self.V[i][j] * hidden_state[j] for j in range(self.hidden_size)) +\n",
        "                      self.output_bias[i][0] for i in range(self.vocab_size)]\n",
        "            probabilities = self.softmax(output)\n",
        "            generated_index = random.choices(range(self.vocab_size), weights=probabilities)[0]\n",
        "            input_state = [0] * self.vocab_size\n",
        "            input_state[generated_index] = 1\n",
        "            generated_indices.append(generated_index)\n",
        "        return generated_indices\n",
        "\n",
        "    # METHOD TO CONVERT STRING TO INTEGER\n",
        "    def encode(self, string_text, string_to_integer_method):\n",
        "        input_state = [0] * self.vocab_size\n",
        "        string = [string for string in string_text]\n",
        "        encoded_to_integer = []\n",
        "\n",
        "        for i in range(len(string)):\n",
        "            int = string_to_integer_method[string[i]]\n",
        "            input_state[int] = 1\n",
        "            encoded_to_integer.append(int)\n",
        "        return encoded_to_integer\n",
        "\n",
        "    # METHOD TO CONVERT INTEGER TO STRING\n",
        "    def decode(self, integer_text, integer_to_string_method):\n",
        "        decoded_to_string = \"\".join(integer_to_string_method[integer] for integer in integer_text)\n",
        "        return decoded_to_string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train Model\n",
        "import json\n",
        "import math\n",
        "\n",
        "# SET HYPERPARAMETERS\n",
        "file_path = \"data.txt\"  #@param {type:\"string\"}\n",
        "sequence_length = 100   #@param {type:\"integer\"}\n",
        "hidden_size = 100  #@param {type:\"integer\"}\n",
        "learning_rate = 5e-5  #@param {type:\"number\"}\n",
        "max_iterations = 500  #@param {type:\"integer\"}\n",
        "convergence_threshold = 0.01  #@param {type:\"number\"}\n",
        "\n",
        "# LOAD DATA AND PREPROCESS\n",
        "with open(file_path, \"r\") as file:\n",
        "    text_data = file.read()\n",
        "vocab = sorted(list(set(text_data)))\n",
        "\n",
        "with open(\"vocab.json\", \"w\") as file:\n",
        "    json.dump(vocab, file, indent=2, ensure_ascii=False)\n",
        "\n",
        "# SIMPLE TOKENIZER\n",
        "with open(\"vocab.json\", \"r\") as file:\n",
        "    vocab = json.load(file)\n",
        "\n",
        "string_to_integer = {string:integer for integer, string in enumerate(vocab)}\n",
        "integer_to_string = {integer:string for integer, string in enumerate(vocab)}\n",
        "\n",
        "data_size = len(text_data)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# INITIALIZE VARIABLES FOR INPUT AND TARGET SEQUENCES\n",
        "input_start = 0\n",
        "input_end = input_start + sequence_length\n",
        "input_indices = [string_to_integer[string] for string in text_data[input_start:input_end]]\n",
        "target_indices = [string_to_integer[string] for string in text_data[input_start + 1:input_end + 1]]\n",
        "input_start += sequence_length\n",
        "if input_start + sequence_length + 1 >= data_size:\n",
        "    input_start = 0\n",
        "\n",
        "# MAKE CONFIGURATION FILE\n",
        "config_dict = {\n",
        "  \"vocab_size\": vocab_size,\n",
        "  \"hidden_size\": hidden_size,\n",
        "  \"sequence_length\": sequence_length,\n",
        "  \"learning_rate\": learning_rate,\n",
        "  \"convergence_threshold\": convergence_threshold\n",
        "}\n",
        "with open(\"config.json\", \"w\") as file:\n",
        "    json.dump(config_dict, file, indent=2)\n",
        "\n",
        "# TRAIN\n",
        "rnn = RecurrentNeuralNetwork(\"config.json\")\n",
        "\n",
        "iteration_number = 0\n",
        "smooth_loss = -math.log(1.0 / rnn.vocab_size) * rnn.sequence_length\n",
        "\n",
        "# TRAINING LOOP\n",
        "while smooth_loss > rnn.convergence_threshold and iteration_number <= max_iterations:\n",
        "    previous_hidden_state = [0] * rnn.hidden_size\n",
        "    input_states, hidden_states, predicted_output = rnn.forward_pass(input_indices, previous_hidden_state)\n",
        "    dU, dW, dV, dbias, d_output_bias = rnn.backward_pass(input_states, hidden_states, predicted_output, target_indices)\n",
        "    loss = rnn.calculate_loss(predicted_output, target_indices)\n",
        "\n",
        "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "    previous_hidden_state = hidden_states[rnn.sequence_length - 1]\n",
        "    if iteration_number % 500 == 0:\n",
        "        sample_generation = rnn.generate(input_indices, 200)\n",
        "        decoded_text = rnn.decode(sample_generation, integer_to_string)\n",
        "        print(f\"Iteration: {iteration_number} | {max_iterations}, Loss: {smooth_loss}\")\n",
        "        print(f\"\\nSample Generation:\\n{decoded_text}\\n\")\n",
        "        rnn.save_model(\"model.bin\")\n",
        "    iteration_number += 1\n",
        "print(\"Training is completed!\\n\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lAK2M0BmUfxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Inference Model\n",
        "import json\n",
        "\n",
        "with open(\"vocab.json\", \"r\") as file:\n",
        "    vocab = json.load(file)\n",
        "\n",
        "string_to_integer = {string:integer for integer, string in enumerate(vocab)}\n",
        "integer_to_string = {integer:string for integer, string in enumerate(vocab)}\n",
        "\n",
        "rnn = RecurrentNeuralNetwork(\"config.json\")\n",
        "rnn.load_model(\"model.bin\")\n",
        "seed_text = \"Once upon a time \"  #@param {type:\"string\"}\n",
        "generated_length = 100  #@param {type:\"integer\"}\n",
        "encoded_text = rnn.encode(seed_text, string_to_integer)\n",
        "output = rnn.generate(encoded_text, generated_length)\n",
        "decoded_text = rnn.decode(output, integer_to_string)\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aPbsto4EYhP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Full Code\n",
        "# IMPORT PRE-BUILT MODULES\n",
        "import random\n",
        "import math\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "# DEFINE FUNCTIONS FOR INITIALIZING WEIGHT MATRICES AND VECTORS\n",
        "def initialize_weight_matrix(rows, cols):\n",
        "    return [[random.uniform(-math.sqrt(1. / rows), math.sqrt(1. / rows)) for _ in range(rows)] for _ in range(cols)]\n",
        "\n",
        "def initialize_column_vector(a):\n",
        "    return [[0] for _ in range(a)]\n",
        "\n",
        "def initialize_zero_matrix(rows, cols):\n",
        "    return [[0] * rows for _ in range(cols)]\n",
        "\n",
        "# RECURRENT NEURAL NETWORK ARCHITECTURE\n",
        "class RecurrentNeuralNetwork:\n",
        "    def __init__(self, config_path):\n",
        "        with open(config_path, \"r\") as File:\n",
        "            config_data = json.load(File)\n",
        "\n",
        "        # INITIALIZE NETWORK PARAMETERS BASED ON CONFIGURATION\n",
        "        self.hidden_size = config_data[\"hidden_size\"]\n",
        "        self.vocab_size = config_data[\"vocab_size\"]\n",
        "        self.sequence_length = config_data[\"sequence_length\"]\n",
        "        self.learning_rate = config_data[\"learning_rate\"]\n",
        "        self.convergence_threshold = config_data[\"convergence_threshold\"]\n",
        "        self.U = initialize_weight_matrix(self.vocab_size, self.hidden_size)\n",
        "        self.V = initialize_weight_matrix(self.hidden_size, self.vocab_size)\n",
        "        self.W = initialize_weight_matrix(self.hidden_size, self.hidden_size)\n",
        "        self.bias = initialize_column_vector(self.hidden_size)\n",
        "        self.output_bias = initialize_column_vector(self.vocab_size)\n",
        "        self.memory_U = initialize_zero_matrix(self.vocab_size, self.hidden_size)\n",
        "        self.memory_W = initialize_zero_matrix(self.hidden_size, self.hidden_size)\n",
        "        self.memory_V = initialize_zero_matrix(self.hidden_size, self.vocab_size)\n",
        "        self.memory_bias = initialize_column_vector(self.hidden_size)\n",
        "        self.memory_output_bias = initialize_column_vector(self.vocab_size)\n",
        "\n",
        "    # METHOD TO COMPUTE SOFTMAX ACTIVATION\n",
        "    def softmax(self, x):\n",
        "        probabilities = [math.exp(xi - max(x)) for xi in x]\n",
        "        return [pi / sum(probabilities) for pi in probabilities]\n",
        "\n",
        "    # FORWARD PASS THROUGH THE NETWORK\n",
        "    def forward_pass(self, inputs, previous_hidden_state):\n",
        "        # INITIALIZE DICTIONARIES TO STORE INTERMEDIATE STATES AND OUTPUTS\n",
        "        input_states, hidden_states, output_states, predicted_output = {}, {}, {}, {}\n",
        "        hidden_states[-1] = previous_hidden_state[:] # INITIALIZE PREVIOUS HIDDEN STATE\n",
        "\n",
        "        # ITERATE OVER EACH TIME STEP\n",
        "        for time_step in range(len(inputs)):\n",
        "            # COMPUTE INPUT, HIDDEN, AND OUTPUT STATES\n",
        "            input_states[time_step] = [0] * self.vocab_size\n",
        "            input_states[time_step][inputs[time_step]] = 1\n",
        "            hidden_states[time_step] = [math.tanh(\n",
        "                sum(self.U[i][j] * input_states[time_step][j] for j in range(self.vocab_size)) +\n",
        "                sum(self.W[i][j] * hidden_states[time_step - 1][j] for j in range(self.hidden_size)) +\n",
        "                self.bias[i][0]) for i in range(self.hidden_size)]\n",
        "            output_states[time_step] = [sum(self.V[i][j] * hidden_states[time_step][j] for j in range(self.hidden_size)) +\n",
        "                                        self.output_bias[i][0] for i in range(self.vocab_size)]\n",
        "            predicted_output[time_step] = self.softmax(output_states[time_step])\n",
        "        return input_states, hidden_states, predicted_output\n",
        "\n",
        "    # BACKWARD PASS THROUGH THE NETWORK\n",
        "    def backward_pass(self, input_states, hidden_states, predicted_output, targets):\n",
        "        # INITIALIZE GRADIENT MATRICES AND VECTORS\n",
        "        dU, dW, dV = [[0] * self.vocab_size for _ in range(self.hidden_size)], [[0] * self.hidden_size for _ in\n",
        "                                                                              range(self.hidden_size)], [\n",
        "                         [0] * self.hidden_size for _ in range(self.vocab_size)]\n",
        "        dbias, d_output_bias = [[0] for _ in range(self.hidden_size)], [[0] for _ in range(self.vocab_size)]\n",
        "        hidden_state_update = [0] * self.hidden_size\n",
        "\n",
        "        # ITERATE OVER EACH TIME STEP IN REVERSE ORDER\n",
        "        for time_step in reversed(range(self.sequence_length)):\n",
        "            output_error = predicted_output[time_step][:]\n",
        "            output_error[targets[time_step]] -= 1\n",
        "\n",
        "            for i in range(self.vocab_size):\n",
        "                for j in range(self.hidden_size):\n",
        "                    dV[i][j] += output_error[i] * hidden_states[time_step][j]\n",
        "                d_output_bias[i][0] += output_error[i]\n",
        "            hidden_error = [sum(self.V[i][j] * output_error[i] for i in range(self.vocab_size)) + hidden_state_update[j]\n",
        "                            for j in range(self.hidden_size)]\n",
        "            hidden_state_recurrent_error = [(1 - hidden_states[time_step][j] * hidden_states[time_step][j]) *\n",
        "                                            hidden_error[j] for j in range(self.hidden_size)]\n",
        "\n",
        "            for i in range(self.hidden_size):\n",
        "                dbias[i][0] += hidden_state_recurrent_error[i]\n",
        "                for j in range(self.vocab_size):\n",
        "                    dU[i][j] += hidden_state_recurrent_error[i] * input_states[time_step][j]\n",
        "                for j in range(self.hidden_size):\n",
        "                    dW[i][j] += hidden_state_recurrent_error[i] * hidden_states[time_step - 1][j]\n",
        "            hidden_state_update = [sum(self.W[i][j] * hidden_state_recurrent_error[j] for j in range(self.hidden_size))\n",
        "                                   for i in range(self.hidden_size)]\n",
        "\n",
        "        # UPDATE PARAMETERS USING RMSPROP OPTIMIZATION\n",
        "        for parameter_updates, parameters, memory in zip([dU, dW, dV, dbias, d_output_bias],\n",
        "                                                        [self.U, self.W, self.V, self.bias, self.output_bias],\n",
        "                                                        [self.memory_U, self.memory_W, self.memory_V, self.memory_bias,\n",
        "                                                         self.memory_output_bias]):\n",
        "            for i in range(len(parameters)):\n",
        "                for j in range(len(parameters[0])):\n",
        "                    parameter_updates[i][j] = max(-5, min(5, parameter_updates[i][j]))\n",
        "                    memory[i][j] += parameter_updates[i][j] * parameter_updates[i][j]\n",
        "                    parameters[i][j] += -self.learning_rate * parameter_updates[i][j] / math.sqrt(\n",
        "                        memory[i][j] + 1e-8)\n",
        "        return dU, dW, dV, dbias, d_output_bias\n",
        "\n",
        "    # METHOD TO CALCULATE LOSS\n",
        "    def calculate_loss(self, predicted_output, targets):\n",
        "        return sum(-math.log(predicted_output[time_step][targets[time_step]]) for time_step in\n",
        "                   range(self.sequence_length))\n",
        "\n",
        "    # METHOD TO SAVE MODEL PARAMETERS\n",
        "    def save_model(self, save_path):\n",
        "        model_data = {\n",
        "            \"U\": self.U,\n",
        "            \"V\": self.V,\n",
        "            \"W\": self.W,\n",
        "            \"bias\": self.bias,\n",
        "            \"output_bias\": self.output_bias\n",
        "        }\n",
        "        with open(save_path, \"wb\") as file:\n",
        "            pickle.dump(model_data, file)\n",
        "\n",
        "    # METHOD TO LOAD MODEL PARAMETERS\n",
        "    def load_model(self, file_path):\n",
        "        with open(file_path, \"rb\") as file:\n",
        "            model_data = pickle.load(file)\n",
        "\n",
        "        self.U = model_data[\"U\"]\n",
        "        self.V = model_data[\"V\"]\n",
        "        self.W = model_data[\"W\"]\n",
        "        self.bias = model_data[\"bias\"]\n",
        "        self.output_bias = model_data[\"output_bias\"]\n",
        "\n",
        "    # METHOD FOR GENERATING TEXT BASED ON SEED TEXT(INFERENCE FUNCTION)\n",
        "    def generate(self, seed_text, generated_length):\n",
        "        input_state = [0] * self.vocab_size\n",
        "        characters = [a for a in seed_text]\n",
        "        generated_indices = []\n",
        "\n",
        "        for i in range(len(characters)):\n",
        "            index = seed_text[i]\n",
        "            input_state[index] = 1\n",
        "            generated_indices.append(index)\n",
        "        hidden_state = [0] * self.hidden_size\n",
        "\n",
        "        for time_step in range(generated_length):\n",
        "            hidden_state = [math.tanh(\n",
        "                sum(self.U[i][j] * input_state[j] for j in range(self.vocab_size)) +\n",
        "                sum(self.W[i][j] * hidden_state[j] for j in range(self.hidden_size)) +\n",
        "                self.bias[i][0]) for i in range(self.hidden_size)]\n",
        "            output = [sum(self.V[i][j] * hidden_state[j] for j in range(self.hidden_size)) +\n",
        "                      self.output_bias[i][0] for i in range(self.vocab_size)]\n",
        "            probabilities = self.softmax(output)\n",
        "            generated_index = random.choices(range(self.vocab_size), weights=probabilities)[0]\n",
        "            input_state = [0] * self.vocab_size\n",
        "            input_state[generated_index] = 1\n",
        "            generated_indices.append(generated_index)\n",
        "        return generated_indices\n",
        "\n",
        "    # METHOD TO CONVERT STRING TO INTEGER\n",
        "    def encode(self, string_text, string_to_integer_method):\n",
        "        input_state = [0] * self.vocab_size\n",
        "        string = [string for string in string_text]\n",
        "        encoded_to_integer = []\n",
        "\n",
        "        for i in range(len(string)):\n",
        "            int = string_to_integer_method[string[i]]\n",
        "            input_state[int] = 1\n",
        "            encoded_to_integer.append(int)\n",
        "        return encoded_to_integer\n",
        "\n",
        "    # METHOD TO CONVERT INTEGER TO STRING\n",
        "    def decode(self, integer_text, integer_to_string_method):\n",
        "        decoded_to_string = \"\".join(integer_to_string_method[integer] for integer in integer_text)\n",
        "        return decoded_to_string\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # SET HYPERPARAMETERS\n",
        "    file_path = \"data.txt\"  # ENTER YOUR DATA FILE PATH FOR TRAINING\n",
        "    sequence_length = 100  # ENTER LENGTH OF INPUT SEQUENCES\n",
        "    hidden_size = 100  # ENTER SIZE OF THE HIDDEN LAYER\n",
        "    learning_rate = 5e-5  # ENTER LEARNING RATE FOR TRAINING\n",
        "    max_iterations = 10000  # ENTER MAXIMUM NUMBER OF TRAINING ITERATIONS\n",
        "    convergence_threshold = 0.01  # ENTER THRESHOLD FOR CONVERGENCE DURING TRAINING\n",
        "\n",
        "    # LOAD DATA AND PREPROCESS\n",
        "    with open(file_path, \"r\") as file:\n",
        "        text_data = file.read()\n",
        "    vocab = sorted(list(set(text_data)))\n",
        "\n",
        "    with open(\"vocab.json\", \"w\") as file:\n",
        "        json.dump(vocab, file, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # SIMPLE TOKENIZER\n",
        "    with open(\"vocab.json\", \"r\") as file:\n",
        "        vocab = json.load(file)\n",
        "\n",
        "    string_to_integer = {string:integer for integer, string in enumerate(vocab)}\n",
        "    integer_to_string = {integer:string for integer, string in enumerate(vocab)}\n",
        "\n",
        "    data_size = len(text_data)\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # INITIALIZE VARIABLES FOR INPUT AND TARGET SEQUENCES\n",
        "    input_start = 0\n",
        "    input_end = input_start + sequence_length\n",
        "    input_indices = [string_to_integer[string] for string in text_data[input_start:input_end]]\n",
        "    target_indices = [string_to_integer[string] for string in text_data[input_start + 1:input_end + 1]]\n",
        "    input_start += sequence_length\n",
        "    if input_start + sequence_length + 1 >= data_size:\n",
        "        input_start = 0\n",
        "\n",
        "    # MAKE CONFIGURATION FILE\n",
        "    config_dict = {\n",
        "      \"vocab_size\": vocab_size,\n",
        "      \"hidden_size\": hidden_size,\n",
        "      \"sequence_length\": sequence_length,\n",
        "      \"learning_rate\": learning_rate,\n",
        "      \"convergence_threshold\": convergence_threshold\n",
        "    }\n",
        "    with open(\"config.json\", \"w\") as file:\n",
        "        json.dump(config_dict, file, indent=2)\n",
        "\n",
        "    # TRAIN\n",
        "    rnn = RecurrentNeuralNetwork(\"config.json\")\n",
        "\n",
        "    iteration_number = 0\n",
        "    smooth_loss = -math.log(1.0 / rnn.vocab_size) * rnn.sequence_length\n",
        "\n",
        "    # TRAINING LOOP\n",
        "    while smooth_loss > rnn.convergence_threshold and iteration_number <= max_iterations:\n",
        "        previous_hidden_state = [0] * rnn.hidden_size\n",
        "        input_states, hidden_states, predicted_output = rnn.forward_pass(input_indices, previous_hidden_state)\n",
        "        dU, dW, dV, dbias, d_output_bias = rnn.backward_pass(input_states, hidden_states, predicted_output, target_indices)\n",
        "        loss = rnn.calculate_loss(predicted_output, target_indices)\n",
        "\n",
        "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "        previous_hidden_state = hidden_states[rnn.sequence_length - 1]\n",
        "        if iteration_number % 500 == 0:\n",
        "            sample_generation = rnn.generate(input_indices, 200)\n",
        "            decoded_text = rnn.decode(sample_generation, integer_to_string)\n",
        "            print(f\"Iteration: {iteration_number} | {max_iterations}, Loss: {smooth_loss}\")\n",
        "            print(f\"\\nSample Generation:\\n{decoded_text}\\n\")\n",
        "            rnn.save_model(\"model.bin\")\n",
        "        iteration_number += 1\n",
        "    print(\"Training is completed!\\n\")\n",
        "\n",
        "    # INFERENCE\n",
        "    # SIMPLE TOKENIZER\n",
        "    with open(\"vocab.json\", \"r\") as file:\n",
        "        vocab = json.load(file)\n",
        "\n",
        "    string_to_integer = {string:integer for integer, string in enumerate(vocab)}\n",
        "    integer_to_string = {integer:string for integer, string in enumerate(vocab)}\n",
        "\n",
        "    rnn = RecurrentNeuralNetwork(\"config.json\")\n",
        "    rnn.load_model(\"model.bin\")\n",
        "\n",
        "    seed_text = \"Once upon a time \"  # ENTER YOUR OWN TEXT FOR GENERATION\n",
        "    generated_length = 100  # ENTER MAX LENGTH FOR GENERATED TEXT\n",
        "    encoded_text = rnn.encode(seed_text, string_to_integer)\n",
        "    output = rnn.generate(encoded_text, generated_length)\n",
        "    decoded_text = rnn.decode(output, integer_to_string)\n",
        "    print(decoded_text)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yUQapeDIeKKw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}